from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
import os
import tempfile
import asyncio
from datetime import datetime
import logging
import json
from typing import AsyncGenerator

from schemas import *
from services.file_service import FileService
from services.dify_service import DifyService
from services.report_service import ReportService
from services.database_service import DatabaseService

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆç³»ç»Ÿ", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆ›å»ºå¿…è¦ç›®å½•
UPLOAD_DIR = "uploads"
REPORTS_DIR = "reports"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(REPORTS_DIR, exist_ok=True)

# åˆå§‹åŒ–æœåŠ¡
file_service = FileService()
report_service = ReportService()
db_service = DatabaseService()

# å…¨å±€è¿›åº¦å­˜å‚¨ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨Redisç­‰ç¼“å­˜ï¼‰
progress_streams = {}

# ä»ç¯å¢ƒå˜é‡è¯»å–Difyé…ç½®
DIFY_API_KEY = os.getenv('DIFY_API_KEY', 'app-your-api-key-here')
DIFY_BASE_URL = os.getenv('DIFY_BASE_URL', 'https://api.dify.ai/v1')

dify_service = DifyService(api_key=DIFY_API_KEY, base_url=DIFY_BASE_URL)

@app.get("/")
async def root():
    return {"message": "å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆç³»ç»Ÿ API"}

@app.get("/api/progress-stream/{processing_id}")
async def get_progress_stream(processing_id: int):
    """è·å–å®æ—¶è¿›åº¦æµ - Server-Sent Events"""

    async def event_stream() -> AsyncGenerator[str, None]:
        """ç”ŸæˆSSEäº‹ä»¶æµ"""
        try:
            # å‘é€åˆå§‹è¿æ¥ç¡®è®¤
            yield f"data: {json.dumps({'type': 'connected', 'processing_id': processing_id})}\n\n"

            # æŒç»­ç›‘å¬è¿›åº¦å˜åŒ–
            last_progress = -1
            last_step = ""
            max_attempts = 300  # æœ€å¤š5åˆ†é’Ÿ (300 * 1s)
            attempt = 0

            while attempt < max_attempts:
                try:
                    # æŸ¥è¯¢å½“å‰å¤„ç†çŠ¶æ€
                    status = await db_service.get_processing_status(processing_id)

                    if status:
                        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                        logger.info(f"SSEæŸ¥è¯¢åˆ°çŠ¶æ€ - processing_id: {processing_id}, upload_id: {status.upload_id}, step: {status.current_step}, status: {status.status}")

                        # æ£€æŸ¥è¿›åº¦æ˜¯å¦æœ‰å˜åŒ–
                        if (status.progress != last_progress or
                            status.current_step != last_step or
                            status.status in ['completed', 'failed']):

                            # å‘é€è¿›åº¦æ›´æ–°
                            progress_data = {
                                'type': 'progress',
                                'processing_id': processing_id,
                                'upload_id': status.upload_id,
                                'current_step': status.current_step,
                                'status': status.status,
                                'progress': status.progress,
                                'message': status.message,
                                'error_message': status.error_message,
                                'updated_time': status.updated_time.isoformat() if status.updated_time else None
                            }

                            yield f"data: {json.dumps(progress_data)}\n\n"

                            last_progress = status.progress
                            last_step = status.current_step

                            # å¦‚æœå®Œæˆæˆ–å¤±è´¥ï¼Œç»“æŸæµ
                            if status.status in ['completed', 'failed']:
                                yield f"data: {json.dumps({'type': 'finished', 'status': status.status})}\n\n"
                                break

                    attempt += 1
                    await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

                except Exception as e:
                    logger.error(f"è¿›åº¦æµé”™è¯¯: {str(e)}")
                    yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                    break

            # è¶…æ—¶å¤„ç†
            if attempt >= max_attempts:
                yield f"data: {json.dumps({'type': 'timeout', 'message': 'è¿›åº¦ç›‘å¬è¶…æ—¶'})}\n\n"

        except Exception as e:
            logger.error(f"SSEæµé”™è¯¯: {str(e)}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/upload", response_model=UploadResponse)
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """ä¸Šä¼ CSVæ–‡ä»¶å¹¶å¼€å§‹å¤„ç†"""

    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒCSVæ–‡ä»¶æ ¼å¼")

    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {file.filename}")

        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = os.path.join(UPLOAD_DIR, f"{timestamp}_{file.filename}")

        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        file_size = len(content)
        logger.info(f"æ–‡ä»¶ä¿å­˜å®Œæˆï¼Œå¤§å°: {file_size} bytes")

        # åˆ›å»ºæ•°æ®åº“è®°å½•
        logger.info("åˆ›å»ºæ•°æ®åº“è®°å½•...")
        upload_record = await db_service.create_upload_record(file.filename, file_path, file_size)
        processing_status = await db_service.create_processing_status(upload_record.id, "upload")

        logger.info(f"æ•°æ®åº“è®°å½•åˆ›å»ºå®Œæˆ - upload_id: {upload_record.id}, processing_id: {processing_status.id}")
        logger.info(f"å¤„ç†çŠ¶æ€éªŒè¯ - processing_status.upload_id: {processing_status.upload_id}")

        # ç«‹å³æ›´æ–°åˆå§‹çŠ¶æ€
        await db_service.update_processing_status(processing_status.id, "upload", "processing", 5.0, "æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œå¼€å§‹å¤„ç†...")

        # åå°å¤„ç†ä»»åŠ¡
        logger.info("æ·»åŠ åå°å¤„ç†ä»»åŠ¡...")
        background_tasks.add_task(
            process_file_background,
            upload_record.id,
            processing_status.id,
            file_path,
            file.filename
        )

        # è®¡ç®—ä¸Šä¼ è€—æ—¶
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"ä¸Šä¼ æ¥å£å¤„ç†å®Œæˆï¼Œè€—æ—¶: {elapsed:.3f}ç§’ï¼Œå³å°†è¿”å›å“åº”")

        return UploadResponse(
            upload_id=upload_record.id,
            processing_id=processing_status.id,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­..."
        )

    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

async def process_file_background(upload_id: int, processing_id: int, file_path: str, filename: str):
    """åå°æ–‡ä»¶å¤„ç†ä»»åŠ¡ - é›†æˆç¤ºä¾‹è„šæœ¬é€»è¾‘"""

    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")

        # æ­¥éª¤1: æ–‡ä»¶ä¸Šä¼ å®Œæˆ
        await db_service.update_processing_status(processing_id, "upload", "processing", 10.0, f"æ–‡ä»¶ {filename} ä¸Šä¼ æˆåŠŸ")

        # æ­¥éª¤2: æ•°æ®æ¸…æ´—
        await db_service.update_processing_status(processing_id, "clean", "processing", 20.0, "å¼€å§‹æ•°æ®æ¸…æ´—...")

        # è¯»å–å’Œæ¸…æ´—CSVæ•°æ®
        raw_data = file_service.read_csv_file(file_path)
        cleaned_data = file_service.clean_data(raw_data)

        # ä¿å­˜åŸå§‹æ•°æ®åˆ°æ•°æ®åº“
        await db_service.save_raw_data(upload_id, raw_data)

        await db_service.update_processing_status(processing_id, "clean", "processing", 30.0, "ä¿ç•™URLã€æ¥æºåç§°ã€ä½œè€…ç”¨æˆ·åç§°ã€æ ‡é¢˜ã€å‘½ä¸­å¥å­ã€è¯­è¨€å­—æ®µ")

        # æ­¥éª¤3: æ•°æ®å»é‡
        await db_service.update_processing_status(processing_id, "dedupe", "processing", 40.0, "å¼€å§‹æ•°æ®å»é‡...")

        before_count = len(cleaned_data)

        # åˆ†æå»é‡å‰çš„æ•°æ®åˆ†å¸ƒ
        analysis = file_service.analyze_hit_sentences(cleaned_data)

        deduplicated_data = await file_service.deduplicate_data(cleaned_data)
        after_count = len(deduplicated_data)
        removed_count = before_count - after_count

        await db_service.update_processing_status(
            processing_id,
            "dedupe",
            "processing",
            50.0,
            f"å»é‡å®Œæˆ - åŸå§‹:{before_count}æ¡ï¼Œä¿ç•™:{after_count}æ¡ï¼Œç§»é™¤:{removed_count}æ¡"
        )

        # ä¿å­˜å»é‡åçš„æ•°æ®åˆ°ä¸´æ—¶CSVæ–‡ä»¶ç”¨äºDifyå¤„ç†
        temp_file_path = file_path.replace('.csv', '_deduplicated.csv')
        await file_service.save_to_csv(deduplicated_data, temp_file_path)

        # æ­¥éª¤4: Difyå·¥ä½œæµå¤„ç†
        await db_service.update_processing_status(processing_id, "workflow", "processing", 60.0, "å¼€å§‹Difyå·¥ä½œæµå¤„ç†...")

        # è°ƒç”¨å¼‚æ­¥Difyå·¥ä½œæµï¼Œä½¿ç”¨å»é‡åçš„æ–‡ä»¶
        workflow_result = await dify_service.process_file_async(temp_file_path, filename.replace('.csv', '_deduplicated.csv'))

        if not workflow_result:
            raise Exception("Difyå·¥ä½œæµå¤„ç†å¤±è´¥")

        # æå–å¢ƒå†…å¤–æ•°æ®æº
        domestic_sources, foreign_sources = dify_service.extract_sources_from_result(workflow_result)

        # æš‚æ—¶æ”¾å®½éªŒè¯æ¡ä»¶ï¼Œé¿å…å› Difyæ¨¡å‹è¿‡è½½å¯¼è‡´çš„å¤±è´¥
        if not domestic_sources and not foreign_sources:
            logger.warning("âš ï¸ Difyè¿”å›ç©ºæ•°æ®ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹è¿‡è½½ï¼‰ï¼Œä½¿ç”¨ç©ºæ•°æ®ç»§ç»­æµç¨‹")
            domestic_sources = []
            foreign_sources = []

        await db_service.update_processing_status(processing_id, "workflow", "processing", 80.0, "æ•°æ®åˆ†ç¦»å’ŒAIåˆ†æå®Œæˆ")

        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        await db_service.save_processed_data(upload_id, domestic_sources, foreign_sources)

        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        await db_service.update_processing_status(processing_id, "report", "processing", 90.0, "å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

        # è®¡ç®—åŸºäºè¯­è¨€çš„å¢ƒå†…å¤–ç»Ÿè®¡æ•°æ®
        raw_data_list = await db_service.get_raw_data_by_upload_id(upload_id)
        inside_count = 0  # è¯­è¨€åŒ…å«Chineseçš„æ•°é‡
        outside_count = 0  # è¯­è¨€ä¸åŒ…å«Chineseçš„æ•°é‡

        for data in raw_data_list:
            if data.language:
                language = data.language.strip()
                if "Chinese" in language:
                    inside_count += 1
                else:
                    outside_count += 1

        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        today = datetime.now()
        date_text = today.strftime('%Yå¹´%mæœˆ%dæ—¥')
        report_filename = f"å—æµ·èˆ†æƒ…æ—¥æŠ¥_{date_text}.docx"
        report_path = os.path.join(REPORTS_DIR, report_filename)

        success = report_service.generate_report(
            domestic_sources,
            foreign_sources,
            inside_total=inside_count,
            outside_total=outside_count,
            output_filename=report_path
        )

        if not success:
            raise Exception("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")

        # æ›´æ–°ä¸Šä¼ è®°å½•
        upload_record = await db_service.get_upload_record(upload_id)
        if upload_record:
            await upload_record.update({
                "report_path": report_path,
                "status": "completed"
            })

        # å®Œæˆå¤„ç†
        logger.info(f"ğŸ‰ å¼€å§‹æœ€ç»ˆçŠ¶æ€æ›´æ–° - processing_id: {processing_id}")
        await db_service.update_processing_status(processing_id, "report", "completed", 100.0, "å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                logger.info(f"ä¸´æ—¶å»é‡æ–‡ä»¶å·²æ¸…ç†: {temp_file_path}")
        except Exception as cleanup_error:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {filename}")

    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            temp_file_path = file_path.replace('.csv', '_deduplicated.csv')
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                logger.info(f"å¼‚å¸¸å¤„ç†ä¸­æ¸…ç†ä¸´æ—¶å»é‡æ–‡ä»¶: {temp_file_path}")
        except Exception as cleanup_error:
            logger.warning(f"å¼‚å¸¸å¤„ç†ä¸­æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        # è·å–å½“å‰å¤„ç†çŠ¶æ€ï¼Œç”¨äºç¡®å®šé”™è¯¯å‘ç”Ÿåœ¨å“ªä¸ªæ­¥éª¤
        current_status = await db_service.get_processing_status(processing_id)
        error_step = current_status.current_step if current_status else "unknown"

        # æ›´æ–°çŠ¶æ€ï¼šå¤±è´¥
        await db_service.update_processing_status(
            processing_id,
            current_step=error_step,
            status="failed",
            error_message=str(e)
        )

        # æ›´æ–°ä¸Šä¼ è®°å½•çŠ¶æ€
        upload_record = await db_service.get_upload_record(upload_id)
        if upload_record:
            await upload_record.update({
                "status": "failed",
                "error_message": str(e)
            })

@app.get("/api/status/{processing_id}", response_model=ProcessingStatusResponse)
async def get_processing_status(processing_id: int):
    """è·å–å¤„ç†çŠ¶æ€"""

    status = await db_service.get_processing_status(processing_id)

    if not status:
        raise HTTPException(status_code=404, detail="å¤„ç†çŠ¶æ€ä¸å­˜åœ¨")

    return ProcessingStatusResponse(
        processing_id=status.id,
        upload_id=status.upload_id,
        current_step=status.current_step,
        status=status.status,
        progress=status.progress,
        message=status.message,
        error_message=status.error_message,
        created_time=status.created_time,
        updated_time=status.updated_time
    )

@app.get("/api/stats/{upload_id}", response_model=DataStatsResponse)
async def get_data_stats(upload_id: int):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""

    stats = await db_service.get_data_stats(upload_id)

    return DataStatsResponse(**stats)

@app.get("/api/data/{upload_id}", response_model=ProcessedDataResponse)
async def get_processed_data(upload_id: int):
    """è·å–å¤„ç†åçš„æ•°æ®"""

    data = await db_service.get_processed_data(upload_id)

    return ProcessedDataResponse(**data)

@app.get("/api/history", response_model=UploadHistoryResponse)
async def get_upload_history(limit: int = 50, offset: int = 0):
    """è·å–ä¸Šä¼ å†å²è®°å½•"""

    return await db_service.get_upload_history(limit, offset)

@app.get("/api/download/{upload_id}")
async def download_report(upload_id: int):
    """ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶"""

    upload_record = await db_service.get_upload_record(upload_id)

    if not upload_record or not upload_record.report_path:
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")

    if not os.path.exists(upload_record.report_path):
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶å·²è¢«åˆ é™¤")

    return FileResponse(
        upload_record.report_path,
        media_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        filename=f"nanhai_report_{upload_id}.docx"
    )

if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)