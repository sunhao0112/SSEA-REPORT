from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
import os
import tempfile
import asyncio
from datetime import datetime
import json
import uuid
from typing import AsyncGenerator

from schemas import *
from services.file_service import FileService
from services.dify_service import DifyService
from services.report_service import ReportService
from services.database_service import DatabaseService
from services.file_security import FileSecurityValidator  # ä¸´æ—¶æ³¨é‡Š
from services.cache_service import cache_manager
from services.cleanup_service import cleanup_service
from services.logger_config import (
    system_logger, api_logger, security_logger, file_logger,
    log_performance, set_request_id, log_stats
)

# åº”ç”¨ç¨‹åºå¯åŠ¨
system_logger.info("å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆç³»ç»Ÿå¯åŠ¨", version="1.0.0")

app = FastAPI(title="å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆç³»ç»Ÿ", version="1.0.0")

# å¯åŠ¨äº‹ä»¶ï¼šå¯åŠ¨åå°æœåŠ¡
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    try:
        # å¯åŠ¨æ¸…ç†æœåŠ¡
        await cleanup_service.start()
        system_logger.info("åå°æœåŠ¡å¯åŠ¨å®Œæˆ")
    except Exception as e:
        system_logger.error("åå°æœåŠ¡å¯åŠ¨å¤±è´¥", error=str(e))

# å…³é—­äº‹ä»¶ï¼šæ¸…ç†èµ„æº
@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    try:
        # åœæ­¢æ¸…ç†æœåŠ¡
        await cleanup_service.stop()
        system_logger.info("åå°æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        system_logger.error("åœæ­¢åå°æœåŠ¡å¤±è´¥", error=str(e))

# è¯·æ±‚ä¸­é—´ä»¶ï¼šæ·»åŠ è¯·æ±‚IDå’Œæ—¥å¿—
@app.middleware("http")
async def log_requests(request: Request, call_next):
    # ç”Ÿæˆè¯·æ±‚ID
    request_id = str(uuid.uuid4())
    set_request_id(request_id)

    start_time = datetime.utcnow()

    # åªè®°å½•é‡è¦çš„è¯·æ±‚å¼€å§‹ï¼ˆä¸Šä¼ ã€å¤„ç†ç­‰ï¼‰
    if request.url.path in ["/api/upload", "/api/cache/clear", "/api/cleanup/manual"]:
        api_logger.info(
            f"Request started: {request.method} {request.url.path}",
            request_id=request_id[:8]  # åªä¿ç•™å‰8ä½
        )

    try:
        response = await call_next(request)
        duration = (datetime.utcnow() - start_time).total_seconds()

        # åªè®°å½•é‡è¦è¯·æ±‚çš„å®Œæˆæˆ–æ…¢è¯·æ±‚
        if (request.url.path in ["/api/upload", "/api/cache/clear", "/api/cleanup/manual"] or
            duration > 1.0):  # è¶…è¿‡1ç§’çš„è¯·æ±‚
            api_logger.info(
                f"Request completed: {request.method} {request.url.path}",
                request_id=request_id[:8],
                status_code=response.status_code,
                duration_seconds=round(duration, 2)
            )

        log_stats.record_request()
        return response

    except Exception as e:
        duration = (datetime.utcnow() - start_time).total_seconds()

        # æ‰€æœ‰é”™è¯¯éƒ½è®°å½•
        api_logger.error(
            f"Request failed: {request.method} {request.url.path}",
            request_id=request_id[:8],
            error_type=type(e).__name__,
            error_message=str(e),
            duration_seconds=round(duration, 2)
        )

        log_stats.record_error(str(e))
        raise

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv('CORS_ORIGINS', 'http://localhost:5173,http://localhost:5174,http://82.157.4.192:8000').split(','),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®å­˜å‚¨ç›®å½• - ä½¿ç”¨ç¯å¢ƒå˜é‡
UPLOAD_DIR = os.getenv('UPLOAD_DIR', './uploads')
REPORTS_DIR = os.getenv('REPORTS_DIR', './reports')
TEMPLATES_DIR = os.getenv('TEMPLATES_DIR', './templates')
LOGS_DIR = os.getenv('LOGS_DIR', './logs')
CACHE_DIR = os.getenv('CACHE_DIR', './cache')

# åˆ›å»ºå¿…è¦ç›®å½•
for directory in [UPLOAD_DIR, REPORTS_DIR, TEMPLATES_DIR, LOGS_DIR, CACHE_DIR]:
    os.makedirs(directory, exist_ok=True)

system_logger.info(
    "å­˜å‚¨ç›®å½•é…ç½®å®Œæˆ",
    upload_dir=UPLOAD_DIR,
    reports_dir=REPORTS_DIR,
    templates_dir=TEMPLATES_DIR,
    logs_dir=LOGS_DIR,
    cache_dir=CACHE_DIR
)

# åˆå§‹åŒ–æœåŠ¡
file_service = FileService()
report_service = ReportService()
db_service = DatabaseService()
file_security = FileSecurityValidator()

# å…¨å±€è¿›åº¦å­˜å‚¨ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨Redisç­‰ç¼“å­˜ï¼‰
progress_streams = {}

# ä»ç¯å¢ƒå˜é‡è¯»å–Difyé…ç½®
DIFY_API_KEY = os.getenv('DIFY_API_KEY', 'app-your-api-key-here')
DIFY_BASE_URL = os.getenv('DIFY_BASE_URL', 'https://api.dify.ai/v1')

dify_service = DifyService(api_key=DIFY_API_KEY, base_url=DIFY_BASE_URL)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼šè¿”å›å‰ç«¯é¡µé¢"""
    static_dir = "/app/static"
    index_path = os.path.join(static_dir, "index.html")

    if os.path.exists(index_path):
        from fastapi.responses import HTMLResponse
        with open(index_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return HTMLResponse(content=content)

    # å¦‚æœæ²¡æœ‰å‰ç«¯æ–‡ä»¶ï¼Œè¿”å›APIä¿¡æ¯
    system_logger.info("è®¿é—®æ ¹è·¯å¾„ - æœªæ‰¾åˆ°å‰ç«¯æ–‡ä»¶")
    return {"message": "å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆç³»ç»Ÿ API", "status": "running", "version": "1.0.0"}

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_status = "ok"
        try:
            await db_service.get_upload_history(limit=1)
        except Exception as e:
            db_status = f"error: {str(e)}"

        # æ£€æŸ¥ç›®å½•çŠ¶æ€
        dirs_status = {
            "upload_dir": os.path.exists(UPLOAD_DIR),
            "reports_dir": os.path.exists(REPORTS_DIR)
        }

        health_info = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": db_status,
            "directories": dirs_status,
            "log_stats": log_stats.get_stats()
        }

        system_logger.info("å¥åº·æ£€æŸ¥å®Œæˆ", **health_info)
        return health_info

    except Exception as e:
        system_logger.error("å¥åº·æ£€æŸ¥å¤±è´¥", error=str(e))
        return {"status": "unhealthy", "error": str(e)}

@app.get("/api/logs/stats")
async def get_log_stats():
    """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
    stats = log_stats.get_stats()
    system_logger.info("è·å–æ—¥å¿—ç»Ÿè®¡", **stats)
    return stats

@app.get("/api/cache/stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    stats = cache_manager.get_stats()
    system_logger.info("è·å–ç¼“å­˜ç»Ÿè®¡", **stats)
    return stats

@app.post("/api/cache/clear")
async def clear_cache(cache_type: str = "all"):
    """æ¸…ç©ºç¼“å­˜"""
    try:
        result = await cache_manager.clear(cache_type)
        system_logger.info("æ¸…ç©ºç¼“å­˜", cache_type=cache_type, success=result)
        return {"success": result, "message": f"å·²æ¸…ç©º {cache_type} ç¼“å­˜"}
    except Exception as e:
        system_logger.error("æ¸…ç©ºç¼“å­˜å¤±è´¥", cache_type=cache_type, error=str(e))
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")

@app.get("/api/cleanup/stats")
async def get_cleanup_stats():
    """è·å–æ¸…ç†æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    stats = cleanup_service.get_stats()
    system_logger.info("è·å–æ¸…ç†ç»Ÿè®¡", **{k: v for k, v in stats.items() if k != 'scheduled_jobs'})
    return stats

@app.post("/api/cleanup/manual")
async def manual_cleanup(operation: str = "full"):
    """æ‰‹åŠ¨è§¦å‘æ¸…ç†"""
    try:
        result = await cleanup_service.manual_cleanup(operation)
        system_logger.info("æ‰‹åŠ¨æ¸…ç†å®Œæˆ", operation=operation, result=result)
        return result
    except Exception as e:
        system_logger.error("æ‰‹åŠ¨æ¸…ç†å¤±è´¥", operation=operation, error=str(e))
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")

@app.get("/api/progress-stream/{processing_id}")
async def get_progress_stream(processing_id: int):
    """è·å–å®æ—¶è¿›åº¦æµ - Server-Sent Events"""

    async def event_stream() -> AsyncGenerator[str, None]:
        """ç”ŸæˆSSEäº‹ä»¶æµ"""
        try:
            # å‘é€åˆå§‹è¿æ¥ç¡®è®¤
            yield f"data: {json.dumps({'type': 'connected', 'processing_id': processing_id})}\n\n"

            # æŒç»­ç›‘å¬è¿›åº¦å˜åŒ–
            last_progress = -1
            last_step = ""
            max_attempts = 300  # æœ€å¤š5åˆ†é’Ÿ (300 * 1s)
            attempt = 0

            while attempt < max_attempts:
                try:
                    # æŸ¥è¯¢å½“å‰å¤„ç†çŠ¶æ€
                    status = await db_service.get_processing_status(processing_id)

                    if status:
                        # æ£€æŸ¥è¿›åº¦æ˜¯å¦æœ‰å˜åŒ–
                        if (status.progress != last_progress or
                            status.current_step != last_step or
                            status.status in ['completed', 'failed']):

                            # å‘é€è¿›åº¦æ›´æ–°
                            progress_data = {
                                'type': 'progress',
                                'processing_id': processing_id,
                                'upload_id': status.upload_id,
                                'current_step': status.current_step,
                                'status': status.status,
                                'progress': status.progress,
                                'message': status.message,
                                'error_message': status.error_message,
                                'updated_time': status.updated_time.isoformat() if status.updated_time else None
                            }

                            yield f"data: {json.dumps(progress_data)}\n\n"

                            last_progress = status.progress
                            last_step = status.current_step

                            # å¦‚æœå®Œæˆæˆ–å¤±è´¥ï¼Œç»“æŸæµ
                            if status.status in ['completed', 'failed']:
                                yield f"data: {json.dumps({'type': 'finished', 'status': status.status})}\n\n"
                                break

                    attempt += 1
                    await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

                except Exception as e:
                    api_logger.error(f"è¿›åº¦æµé”™è¯¯: {str(e)}")
                    yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                    break

            # è¶…æ—¶å¤„ç†
            if attempt >= max_attempts:
                yield f"data: {json.dumps({'type': 'timeout', 'message': 'è¿›åº¦ç›‘å¬è¶…æ—¶'})}\n\n"

        except Exception as e:
            api_logger.error(f"SSEæµé”™è¯¯: {str(e)}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
            "Content-Type": "text/event-stream",
        }
    )

@app.post("/api/upload", response_model=UploadResponse)
@log_performance()
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """ä¸Šä¼ CSVæ–‡ä»¶å¹¶å¼€å§‹å¤„ç† - åŒ…å«å®‰å…¨éªŒè¯"""

    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        file_logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {file.filename}", filename=file.filename)

        # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºå®‰å…¨éªŒè¯
        content = await file.read()
        file_size = len(content)

        # 1. å®‰å…¨éªŒè¯
        security_logger.info(f"å¼€å§‹æ–‡ä»¶å®‰å…¨éªŒè¯: {file.filename}", filename=file.filename, size_bytes=file_size)
        is_valid, error_message, file_info = file_security.validate_file_upload(content, file.filename)

        if not is_valid:
            security_logger.warning(
                f"æ–‡ä»¶å®‰å…¨éªŒè¯å¤±è´¥: {file.filename}",
                filename=file.filename,
                error=error_message,
                file_size=file_size
            )
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶å®‰å…¨éªŒè¯å¤±è´¥: {error_message}")

        security_logger.info(
            f"æ–‡ä»¶å®‰å…¨éªŒè¯é€šè¿‡: {file.filename}",
            filename=file.filename,
            rows_count=file_info.get('rows_count', 0),
            encoding=file_info.get('encoding', 'unknown'),
            hash=file_info.get('hash', '')[:16]
        )

        # 2. æ¸…ç†æ–‡ä»¶åå¹¶ä¿å­˜
        clean_filename = file_security.sanitize_filename(file.filename)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = os.path.join(UPLOAD_DIR, f"{timestamp}_{clean_filename}")

        with open(file_path, "wb") as buffer:
            buffer.write(content)

        file_logger.info(
            f"æ–‡ä»¶ä¿å­˜å®Œæˆ: {clean_filename}",
            filename=clean_filename,
            original_filename=file.filename,
            size_bytes=file_size,
            path=file_path
        )

        # 3. åˆ›å»ºæ•°æ®åº“è®°å½•
        upload_record = await db_service.create_upload_record(
            filename=clean_filename,
            file_path=file_path,
            file_size=file_size
        )
        processing_status = await db_service.create_processing_status(upload_record.id, "upload")

        api_logger.info(
            f"æ•°æ®åº“è®°å½•åˆ›å»ºå®Œæˆ",
            upload_id=upload_record.id,
            processing_id=processing_status.id
        )

        # 4. ç«‹å³æ›´æ–°åˆå§‹çŠ¶æ€
        await db_service.update_processing_status(
            processing_status.id,
            "upload",
            "processing",
            5.0,
            f"æ–‡ä»¶å®‰å…¨éªŒè¯é€šè¿‡ï¼Œå¼€å§‹å¤„ç†... (è¡Œæ•°: {file_info.get('rows_count', 0)})"
        )

        # 5. åå°å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            process_file_background,
            upload_record.id,
            processing_status.id,
            file_path,
            clean_filename
        )

        # è®¡ç®—ä¸Šä¼ è€—æ—¶
        elapsed = (datetime.now() - start_time).total_seconds()
        if elapsed > 2.0:  # åªè®°å½•æ…¢ä¸Šä¼ 
            api_logger.info(
                f"ä¸Šä¼ å¤„ç†è€—æ—¶è¾ƒé•¿: {elapsed:.1f}s",
                upload_id=upload_record.id
            )

        return UploadResponse(
            upload_id=upload_record.id,
            processing_id=processing_status.id,
            message=f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­... (æ£€æµ‹åˆ° {file_info.get('rows_count', 0)} è¡Œæ•°æ®)"
        )

    except HTTPException:
        raise
    except Exception as e:
        api_logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}", error=str(e), filename=file.filename if file else "unknown")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

async def process_file_background(upload_id: int, processing_id: int, file_path: str, filename: str):
    """åå°æ–‡ä»¶å¤„ç†ä»»åŠ¡ - é›†æˆç¤ºä¾‹è„šæœ¬é€»è¾‘"""

    try:
        file_logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}", upload_id=upload_id, processing_id=processing_id)

        # æ­¥éª¤1: æ–‡ä»¶ä¸Šä¼ å®Œæˆ
        await db_service.update_processing_status(processing_id, "upload", "processing", 10.0, f"æ–‡ä»¶ {filename} ä¸Šä¼ æˆåŠŸ")

        # æ­¥éª¤2: æ•°æ®æ¸…æ´—
        await db_service.update_processing_status(processing_id, "clean", "processing", 20.0, "å¼€å§‹æ•°æ®æ¸…æ´—...")

        # è¯»å–å’Œæ¸…æ´—CSVæ•°æ®
        raw_data = file_service.read_csv_file(file_path)
        cleaned_data = file_service.clean_data(raw_data)

        # ä¿å­˜åŸå§‹æ•°æ®åˆ°æ•°æ®åº“
        await db_service.save_raw_data(upload_id, raw_data)

        await db_service.update_processing_status(processing_id, "clean", "processing", 30.0, "ä¿ç•™URLã€æ¥æºåç§°ã€ä½œè€…ç”¨æˆ·åç§°ã€æ ‡é¢˜ã€å‘½ä¸­å¥å­ã€è¯­è¨€å­—æ®µ")

        # æ­¥éª¤3: æ•°æ®å»é‡
        await db_service.update_processing_status(processing_id, "dedupe", "processing", 40.0, "å¼€å§‹æ•°æ®å»é‡...")

        before_count = len(cleaned_data)

        # åˆ†æå»é‡å‰çš„æ•°æ®åˆ†å¸ƒ
        analysis = file_service.analyze_hit_sentences(cleaned_data)

        deduplicated_data = await file_service.deduplicate_data(cleaned_data)
        after_count = len(deduplicated_data)
        removed_count = before_count - after_count

        await db_service.update_processing_status(
            processing_id,
            "dedupe",
            "processing",
            50.0,
            f"å»é‡å®Œæˆ - åŸå§‹:{before_count}æ¡ï¼Œä¿ç•™:{after_count}æ¡ï¼Œç§»é™¤:{removed_count}æ¡"
        )

        # ä¿å­˜å»é‡åçš„æ•°æ®åˆ°ä¸´æ—¶CSVæ–‡ä»¶ç”¨äºDifyå¤„ç†
        temp_file_path = file_path.replace('.csv', '_deduplicated.csv')
        await file_service.save_to_csv(deduplicated_data, temp_file_path)

        # æ­¥éª¤4: Difyå·¥ä½œæµå¤„ç†
        await db_service.update_processing_status(processing_id, "workflow", "processing", 60.0, "å¼€å§‹Difyå·¥ä½œæµå¤„ç†...")

        # è°ƒç”¨å¼‚æ­¥Difyå·¥ä½œæµï¼Œä½¿ç”¨å»é‡åçš„æ–‡ä»¶
        workflow_result = await dify_service.process_file_async(temp_file_path, filename.replace('.csv', '_deduplicated.csv'))

        if not workflow_result:
            raise Exception("Difyå·¥ä½œæµå¤„ç†å¤±è´¥")

        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ç»“æœ
        if isinstance(workflow_result, dict) and 'error' in workflow_result:
            detailed_error = workflow_result.get('error', 'æœªçŸ¥é”™è¯¯')
            await db_service.update_processing_status(
                processing_id,
                "workflow",
                "failed",
                60.0,
                f"Difyå·¥ä½œæµå¤„ç†å¤±è´¥: {detailed_error}"
            )
            raise Exception(f"Difyå·¥ä½œæµå¤„ç†å¤±è´¥: {detailed_error}")

        # æå–å¢ƒå†…å¤–æ•°æ®æº
        domestic_sources, foreign_sources = dify_service.extract_sources_from_result(workflow_result)

        # ç¡®ä¿æ•°æ®æºä¸æ˜¯Noneï¼Œå¦‚æœæ˜¯Noneåˆ™ä½¿ç”¨ç©ºåˆ—è¡¨
        if domestic_sources is None:
            domestic_sources = []
        if foreign_sources is None:
            foreign_sources = []

        # æš‚æ—¶æ”¾å®½éªŒè¯æ¡ä»¶ï¼Œé¿å…å› Difyæ¨¡å‹è¿‡è½½å¯¼è‡´çš„å¤±è´¥
        if not domestic_sources and not foreign_sources:
            file_logger.warning("âš ï¸ Difyè¿”å›ç©ºæ•°æ®ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹è¿‡è½½ï¼‰ï¼Œä½¿ç”¨ç©ºæ•°æ®ç»§ç»­æµç¨‹", upload_id=upload_id)

        await db_service.update_processing_status(processing_id, "workflow", "processing", 80.0, "æ•°æ®åˆ†ç¦»å’ŒAIåˆ†æå®Œæˆ")

        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        await db_service.save_processed_data(upload_id, domestic_sources, foreign_sources)

        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        await db_service.update_processing_status(processing_id, "report", "processing", 90.0, "å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

        # è®¡ç®—åŸºäºè¯­è¨€çš„å¢ƒå†…å¤–ç»Ÿè®¡æ•°æ®
        raw_data_list = await db_service.get_raw_data_by_upload_id(upload_id)
        inside_count = 0  # è¯­è¨€åŒ…å«Chineseçš„æ•°é‡
        outside_count = 0  # è¯­è¨€ä¸åŒ…å«Chineseçš„æ•°é‡

        for data in raw_data_list:
            if data.language and data.language.strip():
                language = data.language.strip()
                if "Chinese" in language:
                    inside_count += 1
                else:
                    outside_count += 1

        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        today = datetime.now()
        date_text = today.strftime('%Yå¹´%mæœˆ%dæ—¥')
        report_filename = f"å—æµ·èˆ†æƒ…æ—¥æŠ¥_{date_text}.docx"
        report_path = os.path.join(REPORTS_DIR, report_filename)

        success = report_service.generate_report(
            domestic_sources,
            foreign_sources,
            inside_total=inside_count,
            outside_total=outside_count,
            output_filename=report_path
        )

        if not success:
            raise Exception("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")

        # æ›´æ–°ä¸Šä¼ è®°å½•
        upload_record = await db_service.get_upload_record(upload_id)
        if upload_record:
            await upload_record.update({
                "report_path": report_path,
                "status": "completed"
            })

        # å®Œæˆå¤„ç†
        file_logger.info(f"ğŸ‰ å¼€å§‹æœ€ç»ˆçŠ¶æ€æ›´æ–°", processing_id=processing_id, upload_id=upload_id)
        await db_service.update_processing_status(processing_id, "report", "completed", 100.0, "å—æµ·èˆ†æƒ…æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                file_logger.info(f"ä¸´æ—¶å»é‡æ–‡ä»¶å·²æ¸…ç†: {temp_file_path}")
        except Exception as cleanup_error:
            file_logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        file_logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: {filename}", upload_id=upload_id)

    except Exception as e:
        import traceback
        file_logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", upload_id=upload_id, error=str(e))
        file_logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}", upload_id=upload_id)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            temp_file_path = file_path.replace('.csv', '_deduplicated.csv')
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                file_logger.info(f"å¼‚å¸¸å¤„ç†ä¸­æ¸…ç†ä¸´æ—¶å»é‡æ–‡ä»¶: {temp_file_path}")
        except Exception as cleanup_error:
            file_logger.warning(f"å¼‚å¸¸å¤„ç†ä¸­æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        # è·å–å½“å‰å¤„ç†çŠ¶æ€ï¼Œç”¨äºç¡®å®šé”™è¯¯å‘ç”Ÿåœ¨å“ªä¸ªæ­¥éª¤
        current_status = await db_service.get_processing_status(processing_id)
        error_step = current_status.current_step if current_status else "unknown"

        # æ›´æ–°çŠ¶æ€ï¼šå¤±è´¥
        await db_service.update_processing_status(
            processing_id,
            current_step=error_step,
            status="failed",
            error_message=str(e)
        )

        # æ›´æ–°ä¸Šä¼ è®°å½•çŠ¶æ€
        upload_record = await db_service.get_upload_record(upload_id)
        if upload_record:
            await upload_record.update({
                "status": "failed",
                "error_message": str(e)
            })

@app.get("/api/status/{processing_id}", response_model=ProcessingStatusResponse)
async def get_processing_status(processing_id: int):
    """è·å–å¤„ç†çŠ¶æ€"""

    status = await db_service.get_processing_status(processing_id)

    if not status:
        raise HTTPException(status_code=404, detail="å¤„ç†çŠ¶æ€ä¸å­˜åœ¨")

    return ProcessingStatusResponse(
        processing_id=status.id,
        upload_id=status.upload_id,
        current_step=status.current_step,
        status=status.status,
        progress=status.progress,
        message=status.message,
        error_message=status.error_message,
        created_time=status.created_time,
        updated_time=status.updated_time
    )

@app.get("/api/stats/{upload_id}", response_model=DataStatsResponse)
async def get_data_stats(upload_id: int):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""

    stats = await db_service.get_data_stats(upload_id)

    return DataStatsResponse(**stats)

@app.get("/api/data/{upload_id}", response_model=ProcessedDataResponse)
async def get_processed_data(upload_id: int):
    """è·å–å¤„ç†åçš„æ•°æ®"""

    data = await db_service.get_processed_data(upload_id)

    return ProcessedDataResponse(**data)

@app.get("/api/history", response_model=UploadHistoryResponse)
async def get_upload_history(limit: int = 50, offset: int = 0):
    """è·å–ä¸Šä¼ å†å²è®°å½•"""

    return await db_service.get_upload_history(limit, offset)

@app.get("/api/download/{upload_id}")
async def download_report(upload_id: int):
    """ä¸‹è½½æŠ¥å‘Šæ–‡ä»¶"""

    upload_record = await db_service.get_upload_record(upload_id)

    if not upload_record or not upload_record.report_path:
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")

    if not os.path.exists(upload_record.report_path):
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶å·²è¢«åˆ é™¤")

    return FileResponse(
        upload_record.report_path,
        media_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        filename=f"nanhai_report_{upload_id}.docx"
    )

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•ï¼ˆå‰ç«¯ï¼‰ - å¿…é¡»åœ¨æ‰€æœ‰APIè·¯ç”±ä¹‹å
static_dir = "/app/static"
if os.path.exists(static_dir):
    # æŒ‚è½½é™æ€èµ„æºç›®å½•
    app.mount("/assets", StaticFiles(directory=f"{static_dir}/assets"), name="assets")
    # æŒ‚è½½å…¶ä»–é™æ€æ–‡ä»¶ï¼ˆå¦‚vite.svgç­‰ï¼‰
    app.mount("/static", StaticFiles(directory=static_dir), name="static_files")

    # å¤„ç†SPAè·¯ç”±çš„catchallè·¯ç”±
    @app.get("/{full_path:path}")
    async def serve_spa(full_path: str):
        """æœåŠ¡å•é¡µåº”ç”¨ï¼Œå¯¹æ‰€æœ‰éAPIè·¯å¾„è¿”å›index.html"""
        # å¦‚æœæ˜¯APIè·¯å¾„ï¼Œè·³è¿‡å¤„ç†
        if full_path.startswith("api/"):
            raise HTTPException(status_code=404, detail="API endpoint not found")

        # å¦‚æœæ˜¯é™æ€æ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡å¤„ç†
        if full_path.startswith(("assets/", "static/")):
            raise HTTPException(status_code=404, detail="Static file not found")

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•çš„é™æ€æ–‡ä»¶ï¼ˆå¦‚vite.svg, favicon.icoç­‰ï¼‰
        static_file_path = os.path.join(static_dir, full_path)
        if os.path.exists(static_file_path) and os.path.isfile(static_file_path):
            return FileResponse(static_file_path)

        # å¦åˆ™è¿”å›index.htmlï¼ˆSPAè·¯ç”±å¤„ç†ï¼‰
        index_path = os.path.join(static_dir, "index.html")
        if os.path.exists(index_path):
            from fastapi.responses import HTMLResponse
            with open(index_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return HTMLResponse(content=content)
        else:
            raise HTTPException(status_code=404, detail="Frontend not found")

    system_logger.info("å‰ç«¯é™æ€æ–‡ä»¶å·²æŒ‚è½½: /app/static")
else:
    system_logger.warning("æœªæ‰¾åˆ°å‰ç«¯é™æ€æ–‡ä»¶ç›®å½•ï¼Œåªæä¾›APIæœåŠ¡")

if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)