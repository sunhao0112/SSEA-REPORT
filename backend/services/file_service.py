import pandas as pd
import numpy as np
import os
from typing import List, Dict, Any
from schemas import RawDataItem

def clean_value(value):
    """æ¸…ç†æ•°æ®å€¼ï¼Œå¤„ç†NaNå’ŒNone"""
    if pd.isna(value) or value is None or (isinstance(value, float) and np.isnan(value)):
        return None
    elif isinstance(value, str) and value.lower() in ['nan', 'null', '']:
        return None
    else:
        return str(value).strip()

class FileService:
    """æ–‡ä»¶å¤„ç†æœåŠ¡"""
    
    def __init__(self):
        self.required_columns = ['URL', 'æ¥æºåç§°', 'ä½œè€…ç”¨æˆ·åç§°', 'æ ‡é¢˜', 'å‘½ä¸­å¥å­', 'è¯­è¨€']
    
    def read_csv_file(self, file_path: str) -> List[Dict[str, Any]]:
        """è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›åŸå§‹æ•°æ®ï¼Œæ”¯æŒå¤šç§ç¼–ç æ ¼å¼å’Œå®¹é”™å¤„ç†"""
        # é¦–å…ˆæ£€æµ‹æ–‡ä»¶ç¼–ç 
        detected_encoding = self._detect_file_encoding(file_path)
        if detected_encoding:
            print(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding}")
        
        # å°è¯•å¤šç§ç¼–ç æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç 
        encodings = ['utf-8', 'utf-8-sig', 'utf-16', 'utf-16le', 'utf-16be', 'gbk', 'gb2312', 'gb18030']
        if detected_encoding and detected_encoding not in encodings:
            encodings.insert(0, detected_encoding)
        
        for encoding in encodings:
            try:
                # å°è¯•ä¸åŒçš„CSVè¯»å–å‚æ•°ç»„åˆ
                read_params = [
                    # æ ‡å‡†å‚æ•°
                    {'encoding': encoding},
                    # å®¹é”™å‚æ•° - å¤„ç†æ ¼å¼ä¸è§„èŒƒçš„CSV
                    {'encoding': encoding, 'error_bad_lines': False, 'warn_bad_lines': True},
                    # æ›´å®½æ¾çš„å‚æ•°
                    {'encoding': encoding, 'sep': None, 'engine': 'python'},
                    # æŒ‡å®šåˆ†éš”ç¬¦
                    {'encoding': encoding, 'sep': ',', 'quotechar': '"', 'skipinitialspace': True},
                    # æœ€å®½æ¾çš„å‚æ•°
                    {'encoding': encoding, 'sep': None, 'engine': 'python', 'on_bad_lines': 'skip'}
                ]
                
                for params in read_params:
                    try:
                        df = pd.read_csv(file_path, **params)
                        if not df.empty:
                            print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶ï¼Œå‚æ•°: {params}")
                            return df.to_dict('records')
                    except Exception:
                        continue
                        
            except UnicodeDecodeError:
                continue
            except Exception as e:
                if encoding == encodings[-1]:  # æœ€åä¸€ä¸ªç¼–ç ä¹Ÿå¤±è´¥äº†
                    # å°è¯•æœ€åçš„å…œåº•æ–¹æ¡ˆï¼šé€è¡Œè¯»å–
                    try:
                        return self._read_csv_line_by_line(file_path, encoding)
                    except Exception:
                        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
                continue
        
        raise Exception("æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç æ ¼å¼ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„CSVæ ¼å¼")
    
    def _read_csv_line_by_line(self, file_path: str, encoding: str) -> List[Dict[str, Any]]:
        """é€è¡Œè¯»å–CSVæ–‡ä»¶çš„å…œåº•æ–¹æ¡ˆ"""
        import csv
        
        data = []
        headers = None
        
        with open(file_path, 'r', encoding=encoding, newline='') as file:
            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            for delimiter in [',', ';', '\t', '|']:
                file.seek(0)
                try:
                    reader = csv.reader(file, delimiter=delimiter)
                    rows = list(reader)
                    
                    if len(rows) > 1 and len(rows[0]) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜è¡Œå’Œä¸€è¡Œæ•°æ®ï¼Œä¸”æœ‰å¤šåˆ—
                        headers = rows[0]
                        for row in rows[1:]:
                            if len(row) >= len(headers):
                                row_dict = {}
                                for i, header in enumerate(headers):
                                    row_dict[header] = row[i] if i < len(row) else ''
                                data.append(row_dict)
                        
                        if data:
                            print(f"ä½¿ç”¨é€è¡Œè¯»å–æˆåŠŸï¼Œåˆ†éš”ç¬¦: '{delimiter}'")
                            return data
                except Exception:
                    continue
        
        raise Exception("æ— æ³•è§£æCSVæ–‡ä»¶æ ¼å¼")
    
    def _detect_file_encoding(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            import chardet
            
            with open(file_path, 'rb') as file:
                # è¯»å–æ–‡ä»¶çš„å‰å‡ KBæ¥æ£€æµ‹ç¼–ç 
                raw_data = file.read(8192)
                result = chardet.detect(raw_data)
                
                if result and result['confidence'] > 0.7:
                    encoding = result['encoding']
                    print(f"chardetæ£€æµ‹ç»“æœ: {encoding} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
                    return encoding

        except ImportError:
            print("chardetæœªå®‰è£…ï¼Œä½¿ç”¨å†…ç½®ç¼–ç æ£€æµ‹")
        except Exception as e:
            print(f"ç¼–ç æ£€æµ‹å¤±è´¥: {e}")
        
        # å†…ç½®ç¼–ç æ£€æµ‹
        try:
            with open(file_path, 'rb') as file:
                raw_data = file.read(4)
                
                # æ£€æµ‹BOM
                if raw_data.startswith(b'\xff\xfe'):
                    return 'utf-16le'
                elif raw_data.startswith(b'\xfe\xff'):
                    return 'utf-16be'
                elif raw_data.startswith(b'\xef\xbb\xbf'):
                    return 'utf-8-sig'
                elif raw_data.startswith(b'\xff\xfe\x00\x00'):
                    return 'utf-32le'
                elif raw_data.startswith(b'\x00\x00\xfe\xff'):
                    return 'utf-32be'
                    
        except Exception as e:
            print(f"âš ï¸ BOMæ£€æµ‹å¤±è´¥: {e}")
        
        return None
    
    def clean_data(self, raw_data: List[Dict[str, Any]]) -> List[RawDataItem]:
        """æ¸…æ´—æ•°æ®ï¼Œåªä¿ç•™éœ€è¦çš„å­—æ®µ"""
        try:
            if not raw_data:
                return []
            
            # è·å–ç¬¬ä¸€è¡Œæ•°æ®æ¥æ£€æŸ¥åˆ—å
            first_row = raw_data[0]
            column_mapping = {}
            missing_columns = []
            
            print(f"ğŸ“‹ CSVæ–‡ä»¶ä¸­çš„åˆ—å: {list(first_row.keys())}")
            print(f"ğŸ¯ éœ€è¦åŒ¹é…çš„åˆ—å: {self.required_columns}")
            
            # å°è¯•åŒ¹é…åˆ—åï¼ˆæ”¯æŒä¸åŒçš„å‘½åæ–¹å¼ï¼‰
            for required_col in self.required_columns:
                found = False
                for col in first_row.keys():
                    if self._is_column_match(col, required_col):
                        column_mapping[required_col] = col
                        print(f"âœ… åŒ¹é…æˆåŠŸ: '{required_col}' -> '{col}'")
                        found = True
                        break
                
                if not found:
                    missing_columns.append(required_col)
                    print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…: '{required_col}'")
            
            if missing_columns:
                print(f"âŒ ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
                print(f"ğŸ’¡ è¯·æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦åŒ…å«ä»¥ä¸‹åˆ—åï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰:")
                for col in missing_columns:
                    print(f"   - {col}")
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
            
            # æå–éœ€è¦çš„æ•°æ®
            cleaned_data = []
            for row in raw_data:
                item = RawDataItem(
                    url=clean_value(row.get(column_mapping['URL'])),
                    source_name=clean_value(row.get(column_mapping['æ¥æºåç§°'])),
                    author_username=clean_value(row.get(column_mapping['ä½œè€…ç”¨æˆ·åç§°'])),
                    title=clean_value(row.get(column_mapping['æ ‡é¢˜'])),
                    hit_sentence=clean_value(row.get(column_mapping['å‘½ä¸­å¥å­'])),
                    language=clean_value(row.get(column_mapping['è¯­è¨€']))
                )
                cleaned_data.append(item)
            
            return cleaned_data
            
        except Exception as e:
            raise Exception(f"æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
    
    
    async def clean_csv_data(self, file_path: str) -> List[RawDataItem]:
        """æ¸…æ´—CSVæ•°æ®ï¼Œåªä¿ç•™éœ€è¦çš„å­—æ®µ"""
        
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(file_path, encoding='utf-8')
            
            # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = []
            column_mapping = {}
            
            # å°è¯•åŒ¹é…åˆ—åï¼ˆæ”¯æŒä¸åŒçš„å‘½åæ–¹å¼ï¼‰
            for required_col in self.required_columns:
                found = False
                for col in df.columns:
                    if self._is_column_match(col, required_col):
                        column_mapping[required_col] = col
                        found = True
                        break
                
                if not found:
                    missing_columns.append(required_col)
            
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
            
            # æå–éœ€è¦çš„æ•°æ®
            cleaned_data = []
            for _, row in df.iterrows():
                item = RawDataItem(
                    url=clean_value(row[column_mapping['URL']]),
                    source_name=clean_value(row[column_mapping['æ¥æºåç§°']]),
                    author_username=clean_value(row[column_mapping['ä½œè€…ç”¨æˆ·åç§°']]),
                    title=clean_value(row[column_mapping['æ ‡é¢˜']]),
                    hit_sentence=clean_value(row[column_mapping['å‘½ä¸­å¥å­']]),
                    language=clean_value(row[column_mapping['è¯­è¨€']])
                )
                cleaned_data.append(item)
            
            return cleaned_data
            
        except Exception as e:
            raise Exception(f"CSVæ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
    
    async def deduplicate_data(self, data: List[RawDataItem]) -> List[RawDataItem]:
        """åŸºäºå‘½ä¸­å¥å­è¿›è¡Œå»é‡"""
        import logging
        logger = logging.getLogger(__name__)

        # è®°å½•å»é‡å‰çš„ç»Ÿè®¡ä¿¡æ¯
        original_count = len(data)
        logger.info(f"ğŸ“Š å»é‡å¤„ç†å¼€å§‹ - åŸå§‹æ•°æ®è¡Œæ•°: {original_count}")

        # ç»Ÿè®¡æœ‰æ— å‘½ä¸­å¥å­çš„æ•°æ®åˆ†å¸ƒ
        has_sentence_count = sum(1 for item in data if item.hit_sentence)
        no_sentence_count = original_count - has_sentence_count
        logger.info(f"ğŸ“ˆ æ•°æ®åˆ†å¸ƒ - æœ‰å‘½ä¸­å¥å­: {has_sentence_count} æ¡ï¼Œæ— å‘½ä¸­å¥å­: {no_sentence_count} æ¡")

        # è°ƒè¯•ï¼šæ‰“å°å‰5ä¸ªå‘½ä¸­å¥å­æ ·ä¾‹
        logger.info("ğŸ” å‰5ä¸ªå‘½ä¸­å¥å­æ ·ä¾‹:")
        for i, item in enumerate(data[:5]):
            if item.hit_sentence:
                sentence_preview = item.hit_sentence.strip()[:100] + '...' if len(item.hit_sentence.strip()) > 100 else item.hit_sentence.strip()
                logger.info(f"   {i+1}. é•¿åº¦:{len(item.hit_sentence.strip())} - {sentence_preview}")
            else:
                logger.info(f"   {i+1}. [ç©ºå‘½ä¸­å¥å­] - æ ‡é¢˜: {item.title}")

        seen_sentences = set()
        deduped_data = []
        duplicate_count = 0
        duplicate_examples = []  # ä¿å­˜å‰å‡ ä¸ªé‡å¤æ ·ä¾‹
        empty_sentence_count = 0

        for i, item in enumerate(data):
            if item.hit_sentence:
                # æ¸…ç†å’Œæ ‡å‡†åŒ–å‘½ä¸­å¥å­ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œç»Ÿä¸€æ¢è¡Œç¬¦ï¼‰
                cleaned_sentence = item.hit_sentence.strip().replace('\r\n', '\n').replace('\r', '\n')

                if not cleaned_sentence:
                    # æ¸…ç†åå‘½ä¸­å¥å­ä¸ºç©ºï¼Œä¿ç•™è¯¥æ¡ç›®
                    empty_sentence_count += 1
                    deduped_data.append(item)
                elif cleaned_sentence not in seen_sentences:
                    seen_sentences.add(cleaned_sentence)
                    deduped_data.append(item)
                else:
                    # è®°å½•é‡å¤æ•°æ®
                    duplicate_count += 1
                    # ä¿å­˜å‰5ä¸ªé‡å¤æ ·ä¾‹ç”¨äºæ—¥å¿—å±•ç¤º
                    if len(duplicate_examples) < 5:
                        duplicate_examples.append({
                            'index': i + 1,
                            'sentence': cleaned_sentence[:100] + '...' if len(cleaned_sentence) > 100 else cleaned_sentence,
                            'sentence_length': len(cleaned_sentence),
                            'url': item.url,
                            'title': item.title[:50] + '...' if item.title and len(item.title) > 50 else item.title
                        })
            else:
                # å¦‚æœæ²¡æœ‰å‘½ä¸­å¥å­ï¼Œä¿ç•™è¯¥æ¡ç›®
                deduped_data.append(item)

        # è®°å½•å»é‡åçš„ç»Ÿè®¡ä¿¡æ¯
        final_count = len(deduped_data)
        removed_count = original_count - final_count
        unique_sentences = len(seen_sentences)

        logger.info(f"âœ… å»é‡å¤„ç†å®Œæˆ - æœ€ç»ˆæ•°æ®è¡Œæ•°: {final_count}")
        logger.info(f"ğŸ—‘ï¸  å»é‡ç»Ÿè®¡ - ç§»é™¤é‡å¤æ•°æ®: {removed_count} æ¡ ({removed_count/original_count*100:.1f}%)")
        logger.info(f"ğŸ”¢ å”¯ä¸€å‘½ä¸­å¥å­æ•°é‡: {unique_sentences}")
        logger.info(f"ğŸ“‹ æ¸…ç†åä¸ºç©ºçš„å‘½ä¸­å¥å­: {empty_sentence_count} æ¡")
        logger.info(f"ğŸ“‹ ä¿ç•™æ•°æ®æ„æˆ - æœ‰æ•ˆå‘½ä¸­å¥å­: {unique_sentences} æ¡ï¼Œç©ºå‘½ä¸­å¥å­: {empty_sentence_count} æ¡ï¼Œæ— å‘½ä¸­å¥å­: {no_sentence_count} æ¡")

        # è¾“å‡ºé‡å¤æ ·ä¾‹
        if duplicate_examples:
            logger.info(f"ğŸ” é‡å¤æ•°æ®æ ·ä¾‹ ({len(duplicate_examples)} ä¸ªç¤ºä¾‹):")
            for example in duplicate_examples:
                logger.info(f"   ç¬¬{example['index']}è¡Œ - é•¿åº¦:{example['sentence_length']} - {example['sentence']}")
                logger.info(f"      æ ‡é¢˜: {example['title'] or 'æ— æ ‡é¢˜'}")
                logger.info(f"      URL: {example['url'] or 'æ— URL'}")

        return deduped_data

    def analyze_hit_sentences(self, data: List[RawDataItem]) -> dict:
        """åˆ†æå‘½ä¸­å¥å­çš„åˆ†å¸ƒæƒ…å†µï¼Œç”¨äºè°ƒè¯•å»é‡é—®é¢˜"""
        import logging
        from collections import Counter
        logger = logging.getLogger(__name__)

        sentence_counts = Counter()
        sentence_examples = {}

        for i, item in enumerate(data):
            if item.hit_sentence:
                cleaned_sentence = item.hit_sentence.strip().replace('\r\n', '\n').replace('\r', '\n')
                if cleaned_sentence:
                    sentence_counts[cleaned_sentence] += 1
                    # ä¿å­˜æ¯ä¸ªå¥å­çš„ç¬¬ä¸€ä¸ªæ ·ä¾‹
                    if cleaned_sentence not in sentence_examples:
                        sentence_examples[cleaned_sentence] = {
                            'first_occurrence': i + 1,
                            'title': item.title,
                            'url': item.url,
                            'source': item.source_name
                        }

        # åˆ†æç»Ÿè®¡ä¿¡æ¯
        total_sentences = len([item for item in data if item.hit_sentence])
        unique_sentences = len(sentence_counts)

        # æ‰¾å‡ºé‡å¤æ¬¡æ•°æœ€å¤šçš„å¥å­
        most_duplicated = sentence_counts.most_common(10)

        analysis = {
            'total_items': len(data),
            'items_with_sentences': total_sentences,
            'unique_sentences': unique_sentences,
            'duplication_rate': (total_sentences - unique_sentences) / total_sentences if total_sentences > 0 else 0,
            'most_duplicated': []
        }

        for sentence, count in most_duplicated:
            if count > 1:
                example = sentence_examples[sentence]
                analysis['most_duplicated'].append({
                    'sentence': sentence[:100] + '...' if len(sentence) > 100 else sentence,
                    'count': count,
                    'first_occurrence': example['first_occurrence'],
                    'title': example['title'],
                    'source': example['source']
                })

        # è®°å½•åˆ†æç»“æœ
        logger.info(f"ğŸ“Š å‘½ä¸­å¥å­åˆ†æç»“æœ:")
        logger.info(f"   æ€»æ•°æ®æ¡æ•°: {analysis['total_items']}")
        logger.info(f"   æœ‰å‘½ä¸­å¥å­çš„æ¡æ•°: {analysis['items_with_sentences']}")
        logger.info(f"   å”¯ä¸€å‘½ä¸­å¥å­æ•°: {analysis['unique_sentences']}")
        logger.info(f"   é¢„æœŸå»é‡ç‡: {analysis['duplication_rate']*100:.1f}%")

        if analysis['most_duplicated']:
            logger.info(f"ğŸ” é‡å¤æ¬¡æ•°æœ€å¤šçš„å‘½ä¸­å¥å­:")
            for dup in analysis['most_duplicated'][:5]:
                logger.info(f"   é‡å¤ {dup['count']} æ¬¡: {dup['sentence']}")
                logger.info(f"      æ¥æº: {dup['source']} | æ ‡é¢˜: {dup['title']}")

        return analysis
    
    def _is_column_match(self, col_name: str, target: str) -> bool:
        """æ£€æŸ¥åˆ—åæ˜¯å¦åŒ¹é…ç›®æ ‡åˆ—å - ä½¿ç”¨ç²¾å‡†åŒ¹é…"""
        col_name = col_name.strip()
        target = target.strip()

        # åªè¿›è¡Œç²¾ç¡®åŒ¹é…
        return col_name == target
    
    async def save_to_csv(self, data: List[RawDataItem], output_path: str):
        """å°†æ¸…æ´—åçš„æ•°æ®ä¿å­˜ä¸ºCSV"""
        
        df_data = []
        for item in data:
            df_data.append({
                'URL': item.url,
                'æ¥æºåç§°': item.source_name,
                'ä½œè€…ç”¨æˆ·åç§°': item.author_username,
                'æ ‡é¢˜': item.title,
                'å‘½ä¸­å¥å­': item.hit_sentence,
                'è¯­è¨€': item.language
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(output_path, index=False, encoding='utf-8')
        
        return output_path