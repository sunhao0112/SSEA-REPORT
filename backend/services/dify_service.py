import aiohttp
import json
from typing import Optional, Dict, Any, Tuple
import logging
import os
import asyncio

logger = logging.getLogger(__name__)

class DifyService:
    def __init__(self, api_key: str, base_url: str = "https://api.dify.ai/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }

    async def upload_file_async(self, file_path: str, filename: str) -> Optional[str]:
        """å¼‚æ­¥ä¸Šä¼ æ–‡ä»¶åˆ°Dify"""
        url = f"{self.base_url}/files/upload"

        try:
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None

            # é…ç½®è¿æ¥å™¨å’Œè¶…æ—¶è®¾ç½®æ¥å¤„ç†å¤§æ–‡ä»¶
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                keepalive_timeout=300,
            )

            timeout = aiohttp.ClientTimeout(
                total=300,  # æ€»è¶…æ—¶æ—¶é—´5åˆ†é’Ÿ
                connect=60,  # è¿æ¥è¶…æ—¶1åˆ†é’Ÿ
                sock_read=120  # è¯»å–è¶…æ—¶2åˆ†é’Ÿ
            )

            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                with open(file_path, 'rb') as f:
                    data = aiohttp.FormData()
                    data.add_field('file', f, filename=filename, content_type='text/csv')

                    headers = {
                        'Authorization': f'Bearer {self.api_key}'
                    }

                    async with session.post(url, headers=headers, data=data) as response:
                        if response.status in [200, 201]:
                            result = await response.json()
                            file_id = result.get('id')
                            if file_id:
                                logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}")
                                return file_id
                            else:
                                logger.error("âŒ å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ID")
                                return None
                        else:
                            error_text = await response.text()
                            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename}")
                            logger.error(f"çŠ¶æ€ç : {response.status}")
                            logger.error(f"é”™è¯¯è¯¦æƒ…: {error_text}")
                            return None

        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    async def run_workflow_async(self, file_id: str) -> Optional[Dict[Any, Any]]:
        """å¼‚æ­¥è¿è¡Œå·¥ä½œæµ - ä½¿ç”¨æµå¼è¾“å‡º"""
        url = f"{self.base_url}/workflows/run"

        payload = {
            "inputs": {
                "raw_data": {
                    "type": "document",
                    "transfer_method": "local_file",
                    "upload_file_id": file_id
                }
            },
            "response_mode": "streaming",
            "user": "abc-123"
        }

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            logger.info("ğŸ”„ å¼€å§‹å¼‚æ­¥æµå¼å¤„ç†å·¥ä½œæµ...")

            # é…ç½®è¿æ¥å™¨å’Œè¶…æ—¶è®¾ç½®æ¥å¤„ç†å¤§æ–‡ä»¶
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                keepalive_timeout=300,
            )

            timeout = aiohttp.ClientTimeout(
                total=600,  # æ€»è¶…æ—¶æ—¶é—´10åˆ†é’Ÿ
                connect=60,  # è¿æ¥è¶…æ—¶1åˆ†é’Ÿ
                sock_read=300  # è¯»å–è¶…æ—¶5åˆ†é’Ÿ
            )

            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        logger.info("ğŸ“¡ æ¥æ”¶æµå¼æ•°æ®...")

                        # å¤„ç†æµå¼æ•°æ®
                        final_result = None
                        total_nodes = 0
                        completed_nodes = 0

                        # ä½¿ç”¨ content.iter_chunked æ¥é¿å… "Chunk too big" é”™è¯¯
                        buffer = b""  # ä½¿ç”¨å­—èŠ‚ç¼“å†²åŒº
                        text_buffer = ""  # æ–‡æœ¬ç¼“å†²åŒº

                        async for chunk in response.content.iter_chunked(8192):  # 8KB chunks
                            if chunk:
                                try:
                                    # å°†æ–°çš„å­—èŠ‚å—æ·»åŠ åˆ°ç¼“å†²åŒº
                                    buffer += chunk

                                    # å°è¯•è§£ç ç¼“å†²åŒºä¸­çš„æ‰€æœ‰å­—èŠ‚
                                    try:
                                        # è§£ç æ•´ä¸ªç¼“å†²åŒº
                                        decoded_text = buffer.decode('utf-8')
                                        # å¦‚æœè§£ç æˆåŠŸï¼Œæ¸…ç©ºå­—èŠ‚ç¼“å†²åŒºï¼Œå°†æ–‡æœ¬æ·»åŠ åˆ°æ–‡æœ¬ç¼“å†²åŒº
                                        buffer = b""
                                        text_buffer += decoded_text
                                    except UnicodeDecodeError as e:
                                        # å¦‚æœè§£ç å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºåœ¨å¤šå­—èŠ‚å­—ç¬¦ä¸­é—´æˆªæ–­äº†
                                        # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„UTF-8å­—ç¬¦è¾¹ç•Œ
                                        if len(buffer) > 0:
                                            # ä»æœ«å°¾å¼€å§‹ï¼Œæ‰¾åˆ°ä¸€ä¸ªå®Œæ•´çš„UTF-8å­—ç¬¦åºåˆ—
                                            for i in range(len(buffer) - 1, max(len(buffer) - 4, -1), -1):
                                                try:
                                                    # å°è¯•è§£ç åˆ°ä½ç½®i
                                                    decoded_text = buffer[:i].decode('utf-8')
                                                    # å¦‚æœæˆåŠŸï¼Œä¿ç•™å®Œæ•´éƒ¨åˆ†ï¼Œæœªå®Œæ•´éƒ¨åˆ†ç•™åœ¨ç¼“å†²åŒº
                                                    text_buffer += decoded_text
                                                    buffer = buffer[i:]
                                                    break
                                                except UnicodeDecodeError:
                                                    continue
                                            else:
                                                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´è¾¹ç•Œï¼Œè·³è¿‡è¿™ä¸ªå—
                                                logger.warning(f"âš ï¸ è·³è¿‡æ— æ³•è§£ç çš„æ•°æ®å—ï¼Œé•¿åº¦: {len(buffer)}")
                                                buffer = b""
                                                continue

                                    # æŒ‰è¡Œåˆ†å‰²å¤„ç†æ–‡æœ¬ç¼“å†²åŒº
                                    while '\n' in text_buffer:
                                        line, text_buffer = text_buffer.split('\n', 1)
                                        line = line.strip()

                                        # è·³è¿‡ç©ºè¡Œå’Œéæ•°æ®è¡Œ
                                        if not line or not line.startswith('data: '):
                                            continue

                                        # æå–JSONæ•°æ®
                                        json_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€

                                        try:
                                            chunk_data = json.loads(json_str)

                                            # å®æ—¶å¤„ç†æµå¼äº‹ä»¶
                                            event = chunk_data.get('event', '')

                                            if event == 'node_started':
                                                node_data = chunk_data.get('data', {})
                                                node_title = node_data.get('title', 'æœªçŸ¥èŠ‚ç‚¹')
                                                # åªè®°å½•å…³é”®èŠ‚ç‚¹
                                                if 'LLM' in node_title or 'æ–‡æ¡£' in node_title:
                                                    logger.info(f"  ğŸ”¸ å¼€å§‹å¤„ç†: {node_title}")
                                                total_nodes += 1

                                            elif event == 'node_finished':
                                                node_data = chunk_data.get('data', {})
                                                node_title = node_data.get('title', 'æœªçŸ¥èŠ‚ç‚¹')
                                                # åªè®°å½•å…³é”®èŠ‚ç‚¹
                                                if 'LLM' in node_title or 'æ–‡æ¡£' in node_title:
                                                    logger.info(f"  âœ… å®Œæˆå¤„ç†: {node_title}")
                                                completed_nodes += 1

                                            elif event == 'workflow_finished':
                                                logger.info("ğŸ‰ å·¥ä½œæµå¤„ç†å®Œæˆ")
                                                final_result = chunk_data

                                                # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å®é™…æˆåŠŸ
                                                data = chunk_data.get('data', {})
                                                workflow_status = data.get('status')

                                                if workflow_status == 'failed':
                                                    error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')
                                                    detailed_error = self._parse_workflow_error({'message': error_msg})
                                                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {detailed_error}")
                                                    return {'error': detailed_error, 'error_data': data, 'raw_error': error_msg}
                                                elif workflow_status in ['running', 'pending']:
                                                    logger.warning(f"âš ï¸ å·¥ä½œæµæœªå®Œæˆï¼ŒçŠ¶æ€: {workflow_status}")
                                                    return {'error': f'å·¥ä½œæµæœªå®Œæˆï¼Œå½“å‰çŠ¶æ€: {workflow_status}', 'status': workflow_status}

                                                break

                                            elif event == 'error':
                                                error_data = chunk_data.get('data', {})
                                                detailed_error = self._parse_workflow_error(error_data)
                                                logger.error(f"âŒ å·¥ä½œæµå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {detailed_error}")
                                                return {'error': detailed_error, 'error_data': error_data}

                                        except json.JSONDecodeError as e:
                                            logger.warning(f"âš ï¸ è§£ææµæ•°æ®å¤±è´¥ (è¡Œé•¿åº¦: {len(json_str)}): {str(e)[:100]}...")
                                            continue

                                except Exception as e:
                                    logger.warning(f"âš ï¸ å¤„ç†chunkæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                                    continue

                        # è¿”å›æœ€ç»ˆç»“æœ
                        if final_result:
                            logger.info(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
                            return final_result
                        else:
                            logger.error("âŒ æœªæ”¶åˆ°å®Œæ•´çš„å·¥ä½œæµç»“æœ")
                            return None

                    else:
                        error_text = await response.text()
                        detailed_error = self._parse_http_error(response.status, error_text)
                        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                        logger.error(f"çŠ¶æ€ç : {response.status}")
                        logger.error(f"è¯¦ç»†é”™è¯¯: {detailed_error}")
                        return {'error': detailed_error, 'status_code': response.status, 'raw_error': error_text}

        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    async def process_file_async(self, file_path: str, filename: str) -> Optional[Dict[Any, Any]]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šä¸Šä¼ å¹¶è¿è¡Œå·¥ä½œæµ"""
        logger.info(f"\nğŸš€ å¼€å§‹å¼‚æ­¥å¤„ç†æ–‡ä»¶: {filename}")

        # ä¸Šä¼ æ–‡ä»¶
        file_id = await self.upload_file_async(file_path, filename)
        if not file_id:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            return {'error': 'æ–‡ä»¶ä¸Šä¼ åˆ°Difyå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ–‡ä»¶æ ¼å¼'}

        # è¿è¡Œå·¥ä½œæµ
        workflow_result = await self.run_workflow_async(file_id)
        if not workflow_result:
            logger.error(f"âŒ æ–‡ä»¶å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            return {'error': 'å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œå¯èƒ½æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜æˆ–APIæœåŠ¡ä¸å¯ç”¨'}

        # å¦‚æœç»“æœæœ¬èº«å°±æ˜¯é”™è¯¯ï¼Œç›´æ¥è¿”å›
        if isinstance(workflow_result, dict) and 'error' in workflow_result:
            return workflow_result

        logger.info(f"âœ… æ–‡ä»¶å¼‚æ­¥å¤„ç†å®Œæˆ")
        return workflow_result

    def extract_sources_from_result(self, workflow_result: Dict[Any, Any]) -> Tuple[Optional[list], Optional[list]]:
        """ä»å·¥ä½œæµç»“æœä¸­æå–å¢ƒå†…å¤–æ•°æ®æº"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯ç»“æœ
            if 'error' in workflow_result:
                logger.error(f"âŒ å·¥ä½œæµè¿”å›é”™è¯¯: {workflow_result['error']}")
                return None, None

            # å®‰å…¨åœ°æ£€æŸ¥æ¯ä¸€çº§æ˜¯å¦å­˜åœ¨
            if not workflow_result:
                logger.error("âŒ workflow_result ä¸ºç©º")
                return None, None

            # è®°å½•å®Œæ•´çš„å·¥ä½œæµç»“æœç»“æ„ä»¥ä¾¿è°ƒè¯•
            logger.info(f"ğŸ“‹ å·¥ä½œæµç»“æœç»“æ„: {list(workflow_result.keys())}")

            data = workflow_result.get('data')
            if not data:
                logger.error("âŒ workflow_result ä¸­æ²¡æœ‰ 'data' å­—æ®µ")
                logger.error(f"âŒ å¯ç”¨å­—æ®µ: {list(workflow_result.keys())}")
                return None, None

            # è®°å½•dataç»“æ„
            logger.info(f"ğŸ“‹ data å­—æ®µç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

            # æ£€æŸ¥å·¥ä½œæµçŠ¶æ€
            workflow_status = data.get('status')
            if workflow_status:
                logger.info(f"ğŸ”„ å·¥ä½œæµçŠ¶æ€: {workflow_status}")

                if workflow_status == 'failed':
                    error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')
                    logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error_msg}")
                    return None, None
                elif workflow_status == 'running':
                    logger.warning("âš ï¸ å·¥ä½œæµä»åœ¨è¿è¡Œä¸­ï¼Œå¯èƒ½æœªå®Œæˆ")
                    return None, None
                elif workflow_status != 'succeeded':
                    logger.warning(f"âš ï¸ å·¥ä½œæµçŠ¶æ€å¼‚å¸¸: {workflow_status}")

            outputs = data.get('outputs')
            if not outputs:
                logger.error("âŒ data ä¸­æ²¡æœ‰ 'outputs' å­—æ®µ")
                logger.error(f"âŒ data ä¸­å¯ç”¨å­—æ®µ: {list(data.keys()) if isinstance(data, dict) else 'dataä¸æ˜¯å­—å…¸ç±»å‹'}")

                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½çš„è¾“å‡ºå­—æ®µ
                possible_output_fields = ['output', 'result', 'response', 'content']
                for field in possible_output_fields:
                    if field in data:
                        logger.info(f"ğŸ” å‘ç°å¯èƒ½çš„è¾“å‡ºå­—æ®µ: {field}")
                        outputs = data[field]
                        break

                if not outputs:
                    return None, None

            # è®°å½•outputsç»“æ„
            logger.info(f"ğŸ“‹ outputs å­—æ®µç»“æ„: {list(outputs.keys()) if isinstance(outputs, dict) else type(outputs)}")

            structured_output = outputs.get('structured_output')
            if not structured_output:
                logger.error("âŒ outputs ä¸­æ²¡æœ‰ 'structured_output' å­—æ®µ")
                logger.error(f"âŒ outputs ä¸­å¯ç”¨å­—æ®µ: {list(outputs.keys()) if isinstance(outputs, dict) else 'outputsä¸æ˜¯å­—å…¸ç±»å‹'}")

                # å°è¯•å…¶ä»–å¯èƒ½çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µå
                possible_structured_fields = ['structured_data', 'parsed_output', 'analysis_result', 'classification']
                for field in possible_structured_fields:
                    if field in outputs:
                        logger.info(f"ğŸ” å‘ç°å¯èƒ½çš„ç»“æ„åŒ–è¾“å‡ºå­—æ®µ: {field}")
                        structured_output = outputs[field]
                        break

                if not structured_output:
                    # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–è¾“å‡ºï¼Œå°è¯•ç›´æ¥ä»outputsè·å–
                    if 'domestic_sources' in outputs or 'foreign_sources' in outputs:
                        logger.info("ğŸ” ç›´æ¥ä»outputsè·å–æ•°æ®æº")
                        domestic_sources = outputs.get('domestic_sources', [])
                        foreign_sources = outputs.get('foreign_sources', [])
                    else:
                        return None, None
                else:
                    domestic_sources = structured_output.get('domestic_sources', [])
                    foreign_sources = structured_output.get('foreign_sources', [])
            else:
                domestic_sources = structured_output.get('domestic_sources', [])
                foreign_sources = structured_output.get('foreign_sources', [])

            # è®°å½•æœ€ç»ˆç»“æœ
            logger.info(f"âœ… æ•°æ®æå–æˆåŠŸ:")
            logger.info(f"- å¢ƒå†…æ¡ç›®æ•°: {len(domestic_sources) if domestic_sources else 0}")
            logger.info(f"- å¢ƒå¤–æ¡ç›®æ•°: {len(foreign_sources) if foreign_sources else 0}")

            return domestic_sources, foreign_sources

        except Exception as e:
            logger.error(f"âŒ æå–æ•°æ®æºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.error(f"âŒ å®Œæ•´å·¥ä½œæµç»“æœ: {workflow_result}")
            return None, None

    def _parse_workflow_error(self, error_data: dict) -> str:
        """è§£æå·¥ä½œæµé”™è¯¯ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†çš„ç”¨æˆ·å‹å¥½é”™è¯¯æè¿°"""
        try:
            # è·å–åŸºç¡€é”™è¯¯ä¿¡æ¯
            message = error_data.get('message', 'æœªçŸ¥é”™è¯¯')
            error_type = error_data.get('error_type', 'æœªçŸ¥ç±»å‹')

            # å°è¯•è§£æåµŒå¥—çš„é”™è¯¯ä¿¡æ¯
            if 'PluginInvokeError' in message:
                # è§£ææ’ä»¶è°ƒç”¨é”™è¯¯
                return self._parse_plugin_error(message)

            # æ£€æŸ¥å¸¸è§çš„APIé”™è¯¯æ¨¡å¼
            if '429' in message or 'RESOURCE_EXHAUSTED' in message:
                return self._parse_quota_error(message)
            elif '401' in message or 'unauthorized' in message.lower():
                return "APIè®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®"
            elif '403' in message or 'forbidden' in message.lower():
                return "APIè®¿é—®è¢«æ‹’ç»ï¼šæƒé™ä¸è¶³æˆ–APIå¯†é’¥æ— æ•ˆ"
            elif '500' in message or 'internal server error' in message.lower():
                return "APIæœåŠ¡å†…éƒ¨é”™è¯¯ï¼šå»ºè®®ç¨åé‡è¯•"
            elif 'timeout' in message.lower():
                return "è¯·æ±‚è¶…æ—¶ï¼šæ•°æ®é‡å¯èƒ½è¿‡å¤§æˆ–ç½‘ç»œè¿æ¥ä¸ç¨³å®š"
            elif 'connection' in message.lower():
                return "ç½‘ç»œè¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°APIæœåŠ¡"

            # è¿”å›åŸå§‹é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæ— æ³•è§£æï¼‰
            return f"{error_type}: {message}"

        except Exception as e:
            logger.warning(f"è§£æå·¥ä½œæµé”™è¯¯å¤±è´¥: {e}")
            return str(error_data)

    def _parse_plugin_error(self, message: str) -> str:
        """è§£ææ’ä»¶è°ƒç”¨é”™è¯¯"""
        try:
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            if 'PluginInvokeError: {' in message:
                json_start = message.find('PluginInvokeError: {') + len('PluginInvokeError: ')
                json_part = message[json_start:]

                # å°è¯•è§£æJSONï¼ˆå¯èƒ½è¢«æˆªæ–­ï¼‰
                try:
                    # æ‰¾åˆ°å®Œæ•´çš„JSONç»“æ„
                    brace_count = 0
                    json_end = 0
                    for i, char in enumerate(json_part):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_end = i + 1
                                break

                    if json_end > 0:
                        json_str = json_part[:json_end]
                        error_obj = json.loads(json_str)
                        return self._parse_plugin_error_object(error_obj)

                except json.JSONDecodeError:
                    pass

            # å¦‚æœæ— æ³•è§£æJSONï¼ŒæŸ¥æ‰¾å…³é”®ä¿¡æ¯
            if '429 RESOURCE_EXHAUSTED' in message:
                return self._parse_quota_error(message)

            return f"æ’ä»¶è°ƒç”¨é”™è¯¯: {message[:200]}..."

        except Exception as e:
            logger.warning(f"è§£ææ’ä»¶é”™è¯¯å¤±è´¥: {e}")
            return f"æ’ä»¶è°ƒç”¨é”™è¯¯: {message[:200]}..."

    def _parse_plugin_error_object(self, error_obj: dict) -> str:
        """è§£ææ’ä»¶é”™è¯¯å¯¹è±¡"""
        error_type = error_obj.get('error_type', 'æœªçŸ¥ç±»å‹')
        message = error_obj.get('message', 'æœªçŸ¥é”™è¯¯')

        if error_type == 'ClientError' and '429' in message:
            return self._parse_quota_error(message)
        elif error_type == 'ClientError' and '401' in message:
            return "APIè®¤è¯é”™è¯¯ï¼šè¯·æ£€æŸ¥Google Gemini APIå¯†é’¥é…ç½®"
        elif error_type == 'ClientError' and '403' in message:
            return "APIè®¿é—®è¢«ç¦æ­¢ï¼šè¯·æ£€æŸ¥APIå¯†é’¥æƒé™æˆ–é¡¹ç›®é…ç½®"
        elif error_type == 'ClientError':
            return f"APIå®¢æˆ·ç«¯é”™è¯¯: {message[:200]}..."
        else:
            return f"{error_type}: {message[:200]}..."

    def _parse_quota_error(self, message: str) -> str:
        """è§£æé…é¢é”™è¯¯ï¼Œæä¾›å…·ä½“çš„è§£å†³å»ºè®®"""
        try:
            # æ£€æŸ¥å…·ä½“çš„é…é¢ç±»å‹
            if 'generate_content_free_tier_input_token_count' in message:
                if 'GenerateContentInputTokensPerModelPerMinute-FreeTier' in message:
                    return ("Google Gemini API å…è´¹é…é¢å·²ç”¨å®Œï¼ˆæ¯åˆ†é’Ÿ250,000 tokensé™åˆ¶ï¼‰\n"
                           "å»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n"
                           "1. ç­‰å¾…1åˆ†é’Ÿåé‡è¯•\n"
                           "2. å‡å°‘å•æ¬¡å¤„ç†çš„æ•°æ®é‡\n"
                           "3. å‡çº§åˆ°ä»˜è´¹ç‰ˆæœ¬ä»¥è·å¾—æ›´é«˜é…é¢")
                else:
                    return ("Google Gemini API é…é¢ä¸è¶³\n"
                           "è¯·æ£€æŸ¥æ‚¨çš„APIé…é¢é™åˆ¶æˆ–å‡çº§è®¡åˆ’")
            elif 'quota' in message.lower() or 'é…é¢' in message:
                return ("APIé…é¢å·²è¾¾ä¸Šé™\n"
                       "å»ºè®®ï¼šç­‰å¾…é…é¢é‡ç½®æˆ–è”ç³»APIæä¾›å•†å‡çº§æœåŠ¡")
            elif '429' in message:
                return ("APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼ˆ429é”™è¯¯ï¼‰\n"
                       "å»ºè®®ï¼šç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•ï¼Œæˆ–å‡å°‘å¹¶å‘è¯·æ±‚æ•°é‡")
            else:
                return f"é…é¢é™åˆ¶é”™è¯¯: {message[:200]}..."

        except Exception as e:
            return "APIé…é¢é”™è¯¯ï¼šè¯·ç¨åé‡è¯•æˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜"

    def _parse_http_error(self, status_code: int, error_text: str) -> str:
        """è§£æHTTPé”™è¯¯å“åº”"""
        try:
            # å°è¯•è§£æJSONé”™è¯¯å“åº”
            try:
                error_obj = json.loads(error_text)
                if isinstance(error_obj, dict):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥ä½œæµé”™è¯¯å“åº”
                    if 'data' in error_obj and 'error' in error_obj['data']:
                        return self._parse_workflow_error(error_obj['data'])
                    elif 'message' in error_obj:
                        return f"APIé”™è¯¯: {error_obj['message']}"
                    elif 'error' in error_obj:
                        if isinstance(error_obj['error'], str):
                            return f"APIé”™è¯¯: {error_obj['error']}"
                        elif isinstance(error_obj['error'], dict):
                            error_msg = error_obj['error'].get('message', str(error_obj['error']))
                            return f"APIé”™è¯¯: {error_msg}"
            except json.JSONDecodeError:
                pass

            # æ ¹æ®HTTPçŠ¶æ€ç æä¾›å‹å¥½é”™è¯¯ä¿¡æ¯
            if status_code == 400:
                return f"è¯·æ±‚å‚æ•°é”™è¯¯ (400): {error_text[:200]}..."
            elif status_code == 401:
                return "APIè®¤è¯å¤±è´¥ (401)ï¼šè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
            elif status_code == 403:
                return "APIè®¿é—®è¢«ç¦æ­¢ (403)ï¼šæƒé™ä¸è¶³æˆ–APIå¯†é’¥æ— æ•ˆ"
            elif status_code == 429:
                return "APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ (429)ï¼šè¯·ç­‰å¾…åé‡è¯•"
            elif status_code == 500:
                return "APIæœåŠ¡å†…éƒ¨é”™è¯¯ (500)ï¼šå»ºè®®ç¨åé‡è¯•"
            elif status_code == 502:
                return "APIç½‘å…³é”™è¯¯ (502)ï¼šæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
            elif status_code == 503:
                return "APIæœåŠ¡ä¸å¯ç”¨ (503)ï¼šæœåŠ¡å™¨ç»´æŠ¤ä¸­"
            elif status_code == 504:
                return "APIè¯·æ±‚è¶…æ—¶ (504)ï¼šæ•°æ®å¤„ç†æ—¶é—´è¿‡é•¿"
            else:
                return f"HTTPé”™è¯¯ ({status_code}): {error_text[:200]}..."

        except Exception as e:
            return f"HTTPé”™è¯¯ ({status_code}): æ— æ³•è§£æé”™è¯¯è¯¦æƒ…"