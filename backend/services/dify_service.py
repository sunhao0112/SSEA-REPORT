import aiohttp
import json
from typing import Optional, Dict, Any, Tuple
import logging
import os
import asyncio

logger = logging.getLogger(__name__)

class DifyService:
    def __init__(self, api_key: str, base_url: str = "https://api.dify.ai/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }

    async def upload_file_async(self, file_path: str, filename: str) -> Optional[str]:
        """å¼‚æ­¥ä¸Šä¼ æ–‡ä»¶åˆ°Dify"""
        url = f"{self.base_url}/files/upload"

        try:
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None

            # é…ç½®è¿æ¥å™¨å’Œè¶…æ—¶è®¾ç½®æ¥å¤„ç†å¤§æ–‡ä»¶
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                keepalive_timeout=300,
            )

            timeout = aiohttp.ClientTimeout(
                total=300,  # æ€»è¶…æ—¶æ—¶é—´5åˆ†é’Ÿ
                connect=60,  # è¿æ¥è¶…æ—¶1åˆ†é’Ÿ
                sock_read=120  # è¯»å–è¶…æ—¶2åˆ†é’Ÿ
            )

            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                with open(file_path, 'rb') as f:
                    data = aiohttp.FormData()
                    data.add_field('file', f, filename=filename, content_type='text/csv')

                    headers = {
                        'Authorization': f'Bearer {self.api_key}'
                    }

                    async with session.post(url, headers=headers, data=data) as response:
                        if response.status in [200, 201]:
                            result = await response.json()
                            file_id = result.get('id')
                            if file_id:
                                logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}")
                                return file_id
                            else:
                                logger.error("âŒ å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ID")
                                return None
                        else:
                            error_text = await response.text()
                            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename}")
                            logger.error(f"çŠ¶æ€ç : {response.status}")
                            logger.error(f"é”™è¯¯è¯¦æƒ…: {error_text}")
                            return None

        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    async def run_workflow_async(self, file_id: str) -> Optional[Dict[Any, Any]]:
        """å¼‚æ­¥è¿è¡Œå·¥ä½œæµ - ä½¿ç”¨æµå¼è¾“å‡º"""
        url = f"{self.base_url}/workflows/run"

        payload = {
            "inputs": {
                "raw_data": {
                    "type": "document",
                    "transfer_method": "local_file",
                    "upload_file_id": file_id
                }
            },
            "response_mode": "streaming",
            "user": "abc-123"
        }

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            logger.info("ğŸ”„ å¼€å§‹å¼‚æ­¥æµå¼å¤„ç†å·¥ä½œæµ...")

            # é…ç½®è¿æ¥å™¨å’Œè¶…æ—¶è®¾ç½®æ¥å¤„ç†å¤§æ–‡ä»¶
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                keepalive_timeout=300,
            )

            timeout = aiohttp.ClientTimeout(
                total=600,  # æ€»è¶…æ—¶æ—¶é—´10åˆ†é’Ÿ
                connect=60,  # è¿æ¥è¶…æ—¶1åˆ†é’Ÿ
                sock_read=300  # è¯»å–è¶…æ—¶5åˆ†é’Ÿ
            )

            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status == 200:
                        logger.info("ğŸ“¡ æ¥æ”¶æµå¼æ•°æ®...")

                        # å¤„ç†æµå¼æ•°æ®
                        final_result = None
                        total_nodes = 0
                        completed_nodes = 0

                        # ä½¿ç”¨ content.iter_chunked æ¥é¿å… "Chunk too big" é”™è¯¯
                        buffer = b""  # ä½¿ç”¨å­—èŠ‚ç¼“å†²åŒº
                        text_buffer = ""  # æ–‡æœ¬ç¼“å†²åŒº

                        async for chunk in response.content.iter_chunked(8192):  # 8KB chunks
                            if chunk:
                                try:
                                    # å°†æ–°çš„å­—èŠ‚å—æ·»åŠ åˆ°ç¼“å†²åŒº
                                    buffer += chunk

                                    # å°è¯•è§£ç ç¼“å†²åŒºä¸­çš„æ‰€æœ‰å­—èŠ‚
                                    try:
                                        # è§£ç æ•´ä¸ªç¼“å†²åŒº
                                        decoded_text = buffer.decode('utf-8')
                                        # å¦‚æœè§£ç æˆåŠŸï¼Œæ¸…ç©ºå­—èŠ‚ç¼“å†²åŒºï¼Œå°†æ–‡æœ¬æ·»åŠ åˆ°æ–‡æœ¬ç¼“å†²åŒº
                                        buffer = b""
                                        text_buffer += decoded_text
                                    except UnicodeDecodeError as e:
                                        # å¦‚æœè§£ç å¤±è´¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºåœ¨å¤šå­—èŠ‚å­—ç¬¦ä¸­é—´æˆªæ–­äº†
                                        # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„UTF-8å­—ç¬¦è¾¹ç•Œ
                                        if len(buffer) > 0:
                                            # ä»æœ«å°¾å¼€å§‹ï¼Œæ‰¾åˆ°ä¸€ä¸ªå®Œæ•´çš„UTF-8å­—ç¬¦åºåˆ—
                                            for i in range(len(buffer) - 1, max(len(buffer) - 4, -1), -1):
                                                try:
                                                    # å°è¯•è§£ç åˆ°ä½ç½®i
                                                    decoded_text = buffer[:i].decode('utf-8')
                                                    # å¦‚æœæˆåŠŸï¼Œä¿ç•™å®Œæ•´éƒ¨åˆ†ï¼Œæœªå®Œæ•´éƒ¨åˆ†ç•™åœ¨ç¼“å†²åŒº
                                                    text_buffer += decoded_text
                                                    buffer = buffer[i:]
                                                    break
                                                except UnicodeDecodeError:
                                                    continue
                                            else:
                                                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´è¾¹ç•Œï¼Œè·³è¿‡è¿™ä¸ªå—
                                                logger.warning(f"âš ï¸ è·³è¿‡æ— æ³•è§£ç çš„æ•°æ®å—ï¼Œé•¿åº¦: {len(buffer)}")
                                                buffer = b""
                                                continue

                                    # æŒ‰è¡Œåˆ†å‰²å¤„ç†æ–‡æœ¬ç¼“å†²åŒº
                                    while '\n' in text_buffer:
                                        line, text_buffer = text_buffer.split('\n', 1)
                                        line = line.strip()

                                        # è·³è¿‡ç©ºè¡Œå’Œéæ•°æ®è¡Œ
                                        if not line or not line.startswith('data: '):
                                            continue

                                        # æå–JSONæ•°æ®
                                        json_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€

                                        try:
                                            chunk_data = json.loads(json_str)

                                            # å®æ—¶å¤„ç†æµå¼äº‹ä»¶
                                            event = chunk_data.get('event', '')

                                            if event == 'node_started':
                                                node_data = chunk_data.get('data', {})
                                                node_title = node_data.get('title', 'æœªçŸ¥èŠ‚ç‚¹')
                                                logger.info(f"  ğŸ”¸ å¼€å§‹å¤„ç†: {node_title}")
                                                total_nodes += 1

                                            elif event == 'node_finished':
                                                node_data = chunk_data.get('data', {})
                                                node_title = node_data.get('title', 'æœªçŸ¥èŠ‚ç‚¹')
                                                logger.info(f"  âœ… å®Œæˆå¤„ç†: {node_title}")
                                                completed_nodes += 1

                                            elif event == 'workflow_finished':
                                                logger.info("ğŸ‰ å·¥ä½œæµå¤„ç†å®Œæˆ")
                                                final_result = chunk_data
                                                break

                                            elif event == 'error':
                                                error_msg = chunk_data.get('data', {}).get('message', 'æœªçŸ¥é”™è¯¯')
                                                logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {error_msg}")
                                                return None

                                        except json.JSONDecodeError as e:
                                            logger.warning(f"âš ï¸ è§£ææµæ•°æ®å¤±è´¥ (è¡Œé•¿åº¦: {len(json_str)}): {str(e)[:100]}...")
                                            continue

                                except Exception as e:
                                    logger.warning(f"âš ï¸ å¤„ç†chunkæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                                    continue

                        # è¿”å›æœ€ç»ˆç»“æœ
                        if final_result:
                            logger.info(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
                            return final_result
                        else:
                            logger.error("âŒ æœªæ”¶åˆ°å®Œæ•´çš„å·¥ä½œæµç»“æœ")
                            return None

                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                        logger.error(f"çŠ¶æ€ç : {response.status}")
                        logger.error(f"é”™è¯¯è¯¦æƒ…: {error_text}")
                        return None

        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œå·¥ä½œæµæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    async def process_file_async(self, file_path: str, filename: str) -> Optional[Dict[Any, Any]]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šä¸Šä¼ å¹¶è¿è¡Œå·¥ä½œæµ"""
        logger.info(f"\nğŸš€ å¼€å§‹å¼‚æ­¥å¤„ç†æ–‡ä»¶: {filename}")

        # ä¸Šä¼ æ–‡ä»¶
        file_id = await self.upload_file_async(file_path, filename)
        if not file_id:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            return None

        # è¿è¡Œå·¥ä½œæµ
        workflow_result = await self.run_workflow_async(file_id)
        if not workflow_result:
            logger.error(f"âŒ æ–‡ä»¶å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
            return None

        logger.info(f"âœ… æ–‡ä»¶å¼‚æ­¥å¤„ç†å®Œæˆ")
        return workflow_result

    def extract_sources_from_result(self, workflow_result: Dict[Any, Any]) -> Tuple[Optional[list], Optional[list]]:
        """ä»å·¥ä½œæµç»“æœä¸­æå–å¢ƒå†…å¤–æ•°æ®æº"""
        try:
            data = workflow_result.get('data', {})
            outputs = data.get('outputs', {})
            structured_output = outputs.get('structured_output', {})

            domestic_sources = structured_output.get('domestic_sources', [])
            foreign_sources = structured_output.get('foreign_sources', [])

            logger.info(f"âœ… æ•°æ®æå–æˆåŠŸ:")
            logger.info(f"- å¢ƒå†…æ¡ç›®æ•°: {len(domestic_sources)}")
            logger.info(f"- å¢ƒå¤–æ¡ç›®æ•°: {len(foreign_sources)}")

            return domestic_sources, foreign_sources

        except Exception as e:
            logger.error(f"âŒ æå–æ•°æ®æºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, None